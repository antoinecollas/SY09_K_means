---
title: "Projet"
output: html_notebook
---

Notebook

Question 1:
```{r}
recettes_pays <- read.csv("donnees/recettes-pays.data", header=T, sep=",")
head(recettes_pays)
recettes_pays_quant = recettes_pays[,-1]
#sum(recettes_pays_quant[1,])
#sum(recettes_pays_quant[,1])
```

```{r}
print(nrow(recettes_pays))
print(length(unique(recettes_pays[,1])))
print(ncol(recettes_pays))
print(min(recettes_pays_quant))
print(max(recettes_pays_quant))
print(sum(is.na(recettes_pays_quant)))
boxplot(recettes_pays_quant)
#summary(recettes_pays)
```

```{r}
library(corrplot)
M <- cor(recettes_pays_quant)
col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
corrplot(M, method = "color") #trop de variables...
```


Question 2:
```{r}
res <- prcomp(recettes_pays_quant)
summary_ACP = summary(res)
print(summary_ACP)
png(filename="imgs/acp.png")
barplot(summary_ACP$importance[2,], las=2)
title(main='Pourcentages d\'inertie expliquée par chaque axe de l\'ACP \n par ordre décroissant.', ylab='pourcentage d\'inertie expliquée')
#(res$sdev)^2 # Les valeurs propres
#res$loadings # Les vecteurs propres
#res$scores # Le nouveau tableau individus-variables
```

Plan 1,2:
```{r}
#ACP_res = data.frame(recettes_pays$origin, res$x, row.names = rownames(recettes_pays))
ACP_res = data.frame(res$x, row.names = rownames(recettes_pays_quant))

png(filename="imgs/acp_plan_1_2.png")
plot(ACP_res[,c(1,2)], type='n', asp=1) # Plan 1,2
text(ACP_res[,1], ACP_res[,2], labels=recettes_pays$origin)
```

Cercle des corrélations pour le plan 1,2:
```{r}
n=dim(recettes_pays_quant)[1]
matrix_recettes_pays_quant <- as.matrix(recettes_pays_quant)
matrix_recettes_pays_quant_centre <- scale(x = matrix_recettes_pays_quant, center = TRUE, scale = FALSE)
V <- (1/n)*t(matrix_recettes_pays_quant_centre) %*% matrix_recettes_pays_quant_centre
tmp <- eigen(V, symmetric=TRUE)
L <- diag(tmp$values)[c(1:n),c(1:n)] #on retire les valeurs propres nulles
print(dim(ACP_res))
correlations <- (1/n) * (t(matrix_recettes_pays_quant_centre)%*%as.matrix(ACP_res)) / (sqrt(diag(V))%*%t(sqrt(diag(L))))
#print(correlations)
print(t(correlations[1,])%*%correlations[1,]) #vérification
print(t(correlations[3,])%*%correlations[3,]) #vérification
corr_1_2 <- correlations[,c(1,2)]
corr_1_3 <- correlations[,c(1,3)]
corr_2_3 <- correlations[,c(2,3)]
```


```{r}
png(filename="imgs/cercle_plan_1_2.png")
plot(corr_1_2, xlim=c(-1.1,1.1), ylim=c(-1.1,1.1), asp=1)
text(correlations[,1], correlations[,2], labels=rownames(correlations), pos=2)
require(plotrix)
draw.circle(0, 0, 1, nv = 1000)
```

Plan 1,3:
```{r}
png(filename="imgs/acp_plan_1_3.png")
plot(ACP_res[,c(1,3)], type='n', asp=1) # Plan 1,3
text(ACP_res[,1], ACP_res[,3], labels=recettes_pays$origin)
```

```{r}
png(filename="imgs/cercle_plan_1_3.png")
plot(corr_1_3, xlim=c(-1.1,1.1), ylim=c(-1.1,1.1), asp=1)
text(correlations[,1], correlations[,3], labels=rownames(correlations), pos=2)
require(plotrix)
draw.circle(0, 0, 1, nv = 1000)
```

Plan 2,3:
```{r}
png(filename="imgs/acp_plan_2_3.png")
plot(ACP_res[,c(2,3)], type='n', asp=1) # Plan 2,3
text(ACP_res[,2], ACP_res[,3], labels=recettes_pays$origin)
```

```{r}
png(filename="imgs/cercle_plan_2_3.png")
plot(corr_2_3, xlim=c(-1.1,1.1), ylim=c(-1.1,1.1), asp=1)
text(correlations[,2], correlations[,3], labels=rownames(correlations), pos=2)
require(plotrix)
draw.circle(0, 0, 1, nv = 1000)
```

Question 3:
```{r}
dist_manhattan_recettes = dist(recettes_pays_quant, method="manhattan")
clust = hclust(dist_manhattan_recettes, method="ward.D2")
print(clust)
png(filename="imgs/CAH_dendrogramme.png")
plot(clust, labels = recettes_pays$origin, xlab = "", ylab = "", main = "Dendrogramme de la classification ascendante hiérarchique")
```

Question 4:
recherche d'un bon K
```{r}
min = vector(length = 10)
for (K in 1:10){
  clust = kmeans(recettes_pays_quant, K)
  #print("K:")
  #print(K)
  temp = clust$tot.withinss
  for (N in 2:100){
    clust = kmeans(recettes_pays_quant, K)
    #print(clust$tot.withinss)
    if (clust$tot.withinss<temp){
      temp = clust$tot.withinss
    }
  }
  min[K]=temp
}
print(min)
png(filename="imgs/K_means_choix_K.png")
plot(min, ylab="Inertie intraclasse", xlab = "K", main = "Inertie intraclasse en fonction du nombre de classes choisi")
```
```{r}
K=3
clust <- kmeans(recettes_pays_quant, K)
#png(filename="imgs/K_means_choix_K.png")
clusters <- data.frame(recettes_pays$origin, clust$cluster)
clusters <- clusters[order(clusters[,2]),] 
print(clusters)
#print(recettes_pays$origin)
#print(clust$cluster)
#plot(clust, main = "K means")
```

Question 6:
```{r}
recettes_echant <- read.csv("donnees/recettes-echant.data", header=T, sep=",")
print(recettes_echant)
```

```{r}
print(nrow(recettes_echant))
print(length(unique(recettes_echant[,1])))
print(ncol(recettes_echant))
recettes_echant_quant <- recettes_echant[,-1]
print(min(recettes_echant_quant))
print(max(recettes_echant_quant))
print(sum(is.na(recettes_echant_quant)))
boxplot(recettes_echant_quant)
#summary(recettes_pays)
```

Question 7:
Pas sur que ce soit bon...
```{r}
aggr <- by(recettes_echant_quant, INDICES=recettes_echant$origin, FUN=colSums)
ing_origin <- as.data.frame(do.call(cbind,aggr))
ing_origin <- ing_origin[order(row.names(ing_origin)),] 
#verification
#print(sum(ing_origin[1,]))
#print(sum(recettes_echant_quant$basil))
nb_recettes_origine = data.frame(t(summary(recettes_echant$origin)))
nb_recettes_origine <- do.call("rbind", replicate(dim(ing_origin)[1], nb_recettes_origine, simplify = FALSE))
ing_origin <- ing_origin / nb_recettes_origine
print(ing_origin)
```

```{r}
dist_ing_origin = dist(ing_origin)
clust = hclust(dist_ing_origin, method="ward.D2")
png(filename="imgs/CAH_dendrogramme_ingredients.png", res=100, height=1000, width=1000)
plot(clust)
```

Deuxième essai de la question 7:
```{r}
ing_origin <- data.frame(t(recettes_echant[,-1]))
ing_origin <- ing_origin[order(row.names(ing_origin)),] 
print(ing_origin)
```

euclidian est sans doute un mauvais choix car si on a deux vecteurs:
0 0 0 0 0 1 0
0 0 1 0 0 0 0
la distance est sqrt(2) ce qui semble faible alors que les deux ingrédients n'ont jamais été utilisés ensemble. La distance euclidienne dépend de la fréquence d'utilisation de deux ingrédients. Deux ingrédients peu utilisés sont proches au sens euclidien car ils sont peu utilisés alors que ce qu'on cherche est à savoir s'ils sont utilisés conjointement.
```{r}
dist_ing_origin = dist(ing_origin, method = "euclidian")
clust = hclust(dist_ing_origin, method="ward.D2")
png(filename="imgs/CAH_dendrogramme_ingredients_euclidian.png", res=100, height=1000, width=1000)
plot(clust)
```

Distance de Jaccard ne prend en compte que quand il y a au moins un 1 dans l'une des deux valeur de chaque couple.
https://fr.wikipedia.org/wiki/Indice_et_distance_de_Jaccard
```{r}
dist_ing_origin = dist(ing_origin, method = "binary")
clust = hclust(dist_ing_origin, method="ward.D2")
png(filename="imgs/CAH_dendrogramme_ingredients_binary.png", res=100, height=1000, width=1000)
plot(clust)
```


Algorithme des k-means avec distance adaptative

```{r}
source(file = "fonctions/distXY.r")

K_means_distance_adaptative <- function(X, K, rho=rep(1, K), n_iter_max=100, n_ess=1, epsilon=10**(-3)) {
  X = as.matrix(X)
  if (K > nrow(X)){
    print("Erreur: le nombre de classes doit être inférieur ou égale au nombre d'individus.")
    return()
  }
  if (length(rho) != K){
    print("Erreur: la longueur du vecteur rho est différente de K.")
    return()
  }
  min_J_star <- Inf
  for (i in 1:n_ess) {
    #initialisaltion
    n <- nrow(X)
    p <- ncol(X)
    essai_suivant <- FALSE
    
    range_1_nb_indiv = 1:n
    sample_1_nb_indiv <- sample(range_1_nb_indiv, size=K)
    mu <- matrix(X[sample_1_nb_indiv,], nrow=K) #les mu_1, mu_2, ..., mu_K sont en ligne pour avoir un tableau individus-variables
    rownames(mu) <- c()
    
    V_tilde <- array(0,c(p,p,K))
    for(k in 1:K){
      V_tilde[,,k] = diag(p)
      V_tilde[,,k] = (rho[k]**(-1/p)) * V_tilde[,,k]
    }
    
    delta = epsilon + 1 #pour faire au moins une itération de l'algorithme
    n_iter = 0
    while((delta>epsilon) && (i<n_iter_max)){
      #print("test0")

      #nouvelle partition
      #pour chaque point on calcule la distance avec les K centres: matrice n,K
      for (k in 1:K) {
        V_tilde_k_inv = solve(V_tilde[,,k], tol=1e-20)
        if(is.na(V_tilde_k_inv) || (sum(eigen(V_tilde_k_inv)$values<1e-20)>=1)){
          essai_suivant <- TRUE
          break()
        }
        distances_au_centre_mu_k <- distXY(X, mu[k,], V_tilde_k_inv) 
        if (k>1){
          distances_aux_centres <- cbind(distances_aux_centres, distances_au_centre_mu_k)
        }else{
          distances_aux_centres <- distances_au_centre_mu_k
        }
      }
      if (essai_suivant){
        break()
      }
      #print("test5")
      #puis on prend la distance la plus faible (sur chaque ligne de la matrice)
      partition = apply(distances_aux_centres, MARGIN=1, FUN = which.min)
      
      #actualisation des paramètres
      mu_old = mu
      #calcul de mu_k
      for(k in 1:K){
        mask <- partition==k
        n_k <- sum(mask)
        mu[k,] <- (1/n_k) * colSums(matrix(X[mask,], nrow=n_k))
      }
      #print("test10")
      #calcul de V_tilde_k
      for(k in 1:K){
        mask <- partition==k
        n_k <- sum(mask)
        mu_k_matrix <- matrix(rep(x=mu[k,], each=n_k), nrow=n_k)
        temp1 <- X[mask,] - mu_k_matrix
        temp2 <- t(temp1)
        temp3 <- apply(temp2,2,function(x) x%*%t(x))
        temp4 <- matrix(rowSums(temp3), nrow = p)#somme sur les lignes
        V_k <- (1/n_k) * temp4
        V_tilde[,,k] = ((rho[k]*det(V_k))**(-1/p)) * V_k
      }
      #print("test20")
      #actualisation des critères de convergence
      delta = 0
      for (k in 1:K) {
        delta = delta + dist(rbind(mu[k,], mu_old[k,]), method = 'euclidian')**2
      }
      n_iter=n_iter+1
      #print("test30")
    }
    if (essai_suivant){
      break()
    }
    #print("test40")
    j_star <- 0
    for (k in 1:K){
      mask <- partition==k
      temp <- apply(X[mask,], 1, function(x) distXY(X = t(x), Y = mu[k,], M = solve(V_tilde[,,k], tol=1e-20)))
      j_star <- j_star + sum(temp)
    }
    
    if (j_star < min_J_star){
      min_J_star <- j_star
      res_n_iter <- n_iter
      res_n_essai <- i

      #calcul de cette nouvelle meilleure partition
      for (k in 1:K) {
        distances_au_centre_mu_k = distXY(X, mu[k,], solve(V_tilde[,,k], tol=1e-20))
        if (k>1){
          distances_aux_centres = cbind(distances_aux_centres, distances_au_centre_mu_k)
        }else{
          distances_aux_centres = distances_au_centre_mu_k
        }
      }
      res_partition = apply(distances_aux_centres, MARGIN=1, FUN = which.min)
      res_mu = mu
      res_V_tilde = V_tilde
    }
  }
  resultats <- list("min_J_star"=min_J_star, "n_iter"=res_n_iter, "partition"=res_partition, "mu"=res_mu, "V_tilde"=res_V_tilde,"n_essai"=res_n_essai)
  return(resultats)
}
```

Données synthétiques

```{r}
library(mclust)
```

```{r}
X <- read.csv("donnees/Synth1.csv", header=T, row.names=1)
z <- X[,3]
X <- X[,-3]

K=2
clust = kmeans(X, K)
clust_adapt <- K_means_distance_adaptative(X, K=K, n_iter_max = 100, n_ess = 10)
png(filename="imgs/K_means_synth_1.png")
plot(X, col=clust$cluster, pch=z)
```
```{r}
png(filename="imgs/K_means_adapt_synth_1.png")
plot(X, col=clust_adapt$partition, pch=z)
print(adjustedRandIndex(z, clust$cluster))
print(adjustedRandIndex(z, clust_adapt$partition))
```

```{r}
X <- read.csv("donnees/Synth2.csv", header=T, row.names=1)
z <- X[,3]
X <- X[,-3]

K=2
clust = kmeans(X, K)
png(filename="imgs/K_means_synth_2.png")
plot(X, col=clust$cluster, pch=z)
```

```{r}
png(filename="imgs/K_means_adapt_synth_2.png")
clust_adapt <- K_means_distance_adaptative(X, K=K, n_iter_max = 100, n_ess = 10)
plot(X, col=clust_adapt$partition, pch=z)
print(adjustedRandIndex(z, clust$cluster))
print(adjustedRandIndex(z, clust_adapt$partition))
```
```{r}
X <- read.csv("donnees/Synth3.csv", header=T, row.names=1)
z <- X[,3]
X <- X[,-3]
X_P1 = X[z==1,]
X_P2 = X[z==2,]

mu_P1 = (1/nrow(X_P1))*colSums(X_P1)
mu_P2 = (1/nrow(X_P2))*colSums(X_P2)

I_1 = 0
for (x in X_P1){
  I_1 = I_1 + (x[1]-mu_P1[1])**2 + (x[2]-mu_P1[2])**2
}
print(I_1)

I_2 = 0
for (x in X_P2){
  I_2 = I_2 + (x[1]-mu_P2[1])**2 + (x[2]-mu_P2[2])**2
}
print(I_2)
```

```{r}
K=2
clust <- kmeans(X, K)
png(filename="imgs/K_means_synth_3.png")
plot(X, col=clust$cluster, pch=z)
```

```{r}
clust_adapt <- K_means_distance_adaptative(X, K=K, n_iter_max = 100, n_ess = 10)
png(filename="imgs/K_means_adapt_synth_3.png")
plot(X, col=clust_adapt$partition, pch=z)
print(adjustedRandIndex(z, clust$cluster))
print(adjustedRandIndex(z, clust_adapt$partition))
```



Données réelles
Iris

```{r}
data(iris)
X <- iris[,1:4]
z <- iris[,5]
```

```{r}
clust_euclidian_inertia <- vector(length = 5)

K=2
clust_euclidian <- kmeans(X, K, nstart = 100)
clust_euclidian_inertia[K] <- clust_euclidian$tot.withinss
png(filename="imgs/K_means_iris_2.png")
plot(X, col=clust_euclidian$cluster, pch=as.numeric(z), main = "Classification des iris de Fisher avec l'algorithme des K-means (distance\n euclidienne, K=2). Les symboles représentent les vraies classes.")
```

```{r}
clust_adaptative_dist <- vector(length = 5)
clust_adaptative <- K_means_distance_adaptative(X, K=K, n_iter_max = 100, n_ess = 100)
clust_adaptative_dist[K] <- clust_adaptative$min_J_star
png(filename="imgs/K_means_adapt_iris_2.png")
plot(X, col=clust_adaptative$partition, pch=as.numeric(z), main = "Classification des iris de Fisher avec l'algorithme des K-means (distance\n adaptative, K=2). Les symboles représentent les vraies classes.")
```

```{r}
K=3
clust_euclidian <- kmeans(X, K, nstart = 100)
clust_euclidian_inertia[K] <- clust_euclidian$tot.withinss
#png(filename="imgs/K_means_choix_K.png")
plot(X, col=clust_euclidian$cluster, pch=as.numeric(z), main = "Classification des iris de Fisher avec l'algorithme des K-means (distance\n euclidienne, K=3). Les symboles représentent les vraies classes.")

clust_adaptative <- K_means_distance_adaptative(X, K=K, n_iter_max = 100, n_ess = 100)
clust_adaptative_dist[K] <- clust_adaptative$min_J_star
#png(filename="imgs/K_means_choix_K.png")
plot(X, col=clust_adaptative$partition, pch=as.numeric(z), main = "Classification des iris de Fisher avec l'algorithme des K-means (distance\n adaptative, K=3). Les symboles représentent les vraies classes.")
```

```{r}
K=4
clust_euclidian <- kmeans(X, K, nstart = 100)
clust_euclidian_inertia[K] <- clust_euclidian$tot.withinss
#png(filename="imgs/K_means_choix_K.png")
plot(X, col=clust_euclidian$cluster, pch=as.numeric(z), main = "Classification des iris de Fisher avec l'algorithme des K-means (distance\n euclidienne, K=4). Les symboles représentent les vraies classes.")

clust_adaptative <- K_means_distance_adaptative(X, K=K, n_iter_max = 100, n_ess = 100)
clust_adaptative_dist[K] <- clust_adaptative$min_J_star
#png(filename="imgs/K_means_choix_K.png")
plot(X, col=clust_adaptative$partition, pch=as.numeric(z), main = "Classification des iris de Fisher avec l'algorithme des K-means (distance\n adaptative, K=4). Les symboles représentent les vraies classes.")
```

```{r}
K=5
clust_euclidian <- kmeans(X, K, nstart = 100)
clust_euclidian_inertia[K] <- clust_euclidian$tot.withinss
#png(filename="imgs/K_means_choix_K.png")
plot(X, col=clust_euclidian$cluster, pch=as.numeric(z), main = "Classification des iris de Fisher avec l'algorithme des K-means (distance\n euclidienne, K=5). Les symboles représentent les vraies classes.")

clust_adaptative <- K_means_distance_adaptative(X, K=K, n_iter_max = 100, n_ess = 100)
clust_adaptative_dist[K] <- clust_adaptative$min_J_star
#png(filename="imgs/K_means_choix_K.png")
plot(X, col=clust_adaptative$partition, pch=as.numeric(z), main = "Classification des iris de Fisher avec l'algorithme des K-means (distance\n adaptative, K=5). Les symboles représentent les vraies classes.")
```
```{r}
#K-means avec distance euclidienne
clust_euclidian_inertia[1] <- kmeans(X, 1, nstart = 10)$tot.withinss
png(filename="imgs/K_means_Iris_critere.png")
plot(clust_euclidian_inertia, ylab="Inertie intraclasse", xlab = "K", main = "Inertie intraclasse en fonction du nombre de classes choisi.", ylim = c(0,700))
```

```{r}
#K-means avec distance adaptative
clust_adaptative_dist[1] <- K_means_distance_adaptative(X, K=1, n_iter_max = 100, n_ess = 10)$min_J_star
png(filename="imgs/K_means_adapt_Iris_critere.png")
plot(clust_adaptative_dist, ylab="Distance", xlab = "K", main = "Distance en fonction du nombre de classes choisi.", ylim=c(0,130))
```

Spam
```{r}
Spam <- read.csv("donnees/spam.csv", header=T, row.names=1)
print(nrow(Spam))
print(ncol(Spam))
X <- Spam[,-58]
z <- Spam[,58]
#print(X)
#print(unique(z))
res <- prcomp(X)
summary_ACP = summary(res)
barplot(summary_ACP$importance[2,])
ACP_spam = data.frame(res$x[,c(1,2)], row.names = rownames(Spam))
plot(ACP_spam, col=z)
```

```{r}
K=2
#ACP_spam <- ACP_spam[mask, ]
#print(dim(ACP_spam))
#z <- z[mask]
clust_euclidian <- kmeans(ACP_spam, K, nstart = 10)
#png(filename="imgs/K_means_choix_K.png")
plot(ACP_spam, col=clust_euclidian$cluster, pch=z)

clust_adaptative <- K_means_distance_adaptative(ACP_spam, K=K, n_iter_max = 100, n_ess = 5)
#png(filename="imgs/K_means_choix_K.png")
plot(ACP_spam, col=clust_adaptative$partition, pch=z)

print(adjustedRandIndex(z, clust_euclidian$cluster))
print(adjustedRandIndex(z, clust_adaptative$partition))

print(data.frame(z))
print(data.frame(clust_adaptative$partition))
```




